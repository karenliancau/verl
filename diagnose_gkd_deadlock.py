#!/usr/bin/env python3
"""
GKD Deadlock Diagnosis Tool
Âø´ÈÄüËØäÊñ≠ GKD ËÆ≠ÁªÉÂç°Ê≠ªÈóÆÈ¢ò
"""

import os
import sys
import subprocess
import time
from datetime import datetime
from pathlib import Path


def print_section(title):
    """ÊâìÂç∞Á´†ËäÇÊ†áÈ¢ò"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")


def run_command(cmd, description=""):
    """ËøêË°åÂëΩ‰ª§Âπ∂ÊçïËé∑ËæìÂá∫"""
    if description:
        print(f"[*] {description}...")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=5)
        return result.stdout + result.stderr
    except subprocess.TimeoutExpired:
        return "[TIMEOUT] Command took too long to execute"
    except Exception as e:
        return f"[ERROR] {str(e)}"


def check_nvidia_gpu():
    """Ê£ÄÊü• NVIDIA GPU Áä∂ÊÄÅ"""
    print_section("1. NVIDIA GPU Status")
    
    # Ê£ÄÊü• nvidia-smi ÊòØÂê¶ÂèØÁî®
    output = run_command("nvidia-smi", "Running nvidia-smi")
    print(output)
    
    # Ê£ÄÊü• GPU ÂÜÖÂ≠ò
    output = run_command(
        "nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader",
        "Checking GPU memory"
    )
    print("GPU Memory Usage:")
    print(output)
    
    # Ê£ÄÊü•ËøõÁ®ãÂç†Áî®ÁöÑ GPU
    output = run_command(
        "nvidia-smi pmon -c 1",
        "Checking GPU processes"
    )
    print("Active GPU Processes:")
    print(output)


def check_ray_cluster():
    """Ê£ÄÊü• Ray ÈõÜÁæ§Áä∂ÊÄÅ"""
    print_section("2. Ray Cluster Status")
    
    code = """
import ray
import json

try:
    if not ray.is_initialized():
        print("Ray is NOT initialized")
        sys.exit(0)
    
    print("Ray is initialized")
    print(f"Dashboard URL: {ray.get_dashboard_url()}")
    
    # Ê£ÄÊü•ÈõÜÁæ§ËµÑÊ∫ê
    resources = ray.cluster_resources()
    print(f"\\nCluster Resources:")
    for key, value in resources.items():
        print(f"  {key}: {value}")
    
    # Ê£ÄÊü•ËäÇÁÇπ‰ø°ÊÅØ
    print(f"\\nCluster Nodes:")
    nodes = ray.nodes()
    for i, node in enumerate(nodes):
        print(f"  Node {i}:")
        print(f"    NodeID: {node['NodeID']}")
        print(f"    RayletIP: {node['RayletIP']}")
        print(f"    Resources: {node['Resources']}")
    
    # Ê£ÄÊü• Actor ‰ø°ÊÅØ
    print(f"\\nActive Actors:")
    try:
        actors = ray.util.list_named_actors()
        for actor in actors:
            print(f"  {actor['name']}: {actor['state']}")
    except Exception as e:
        print(f"  Cannot list actors: {e}")
        
except Exception as e:
    print(f"Error checking Ray: {e}")
    import traceback
    traceback.print_exc()
"""
    
    output = run_command(
        f"python3 -c \"{code}\"",
        "Checking Ray cluster"
    )
    print(output)


def check_torch_distributed():
    """Ê£ÄÊü• PyTorch ÂàÜÂ∏ÉÂºèÂàùÂßãÂåñ"""
    print_section("3. PyTorch Distributed Status")
    
    code = """
import torch
import os

print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"Number of GPUs: {torch.cuda.device_count()}")
    print(f"GPU Names:")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f}GB")

print(f"\\nDistributed Environment Variables:")
dist_vars = [
    'RANK', 'WORLD_SIZE', 'LOCAL_RANK', 'MASTER_ADDR', 'MASTER_PORT',
    'NCCL_DEBUG', 'TORCH_DISTRIBUTED_DEBUG', 'NCCL_TIMEOUT'
]
for var in dist_vars:
    value = os.environ.get(var, "NOT SET")
    print(f"  {var}: {value}")

print(f"\\nNCCL Environment Variables:")
nccl_vars = [k for k in os.environ.keys() if k.startswith('NCCL_')]
for var in nccl_vars:
    print(f"  {var}: {os.environ[var]}")

print(f"\\nDistributed Initialized: {torch.distributed.is_initialized()}")
"""
    
    output = run_command(
        f"python3 -c \"{code}\"",
        "Checking PyTorch distributed"
    )
    print(output)


def check_verl_imports():
    """Ê£ÄÊü• VERL Ê®°ÂùóÂØºÂÖ•"""
    print_section("4. VERL Module Imports")
    
    code = """
import sys

modules_to_check = [
    'verl',
    'verl.workers',
    'verl.single_controller',
    'recipe.gkd',
    'recipe.gkd.ray_trainer',
    'recipe.gkd.megatron_workers',
    'recipe.gkd.teacher',
]

for module in modules_to_check:
    try:
        __import__(module)
        print(f"‚úì {module}")
    except ImportError as e:
        print(f"‚úó {module}: {e}")
    except Exception as e:
        print(f"‚úó {module}: {type(e).__name__}: {e}")
"""
    
    output = run_command(
        f"python3 -c \"{code}\"",
        "Checking VERL imports"
    )
    print(output)


def check_config():
    """Ê£ÄÊü•ÈÖçÁΩÆÊñá‰ª∂"""
    print_section("5. Configuration Files")
    
    config_file = "recipe/gkd/config/on_policy_distill_trainer.yaml"
    if Path(config_file).exists():
        print(f"‚úì Found {config_file}")
        output = run_command(
            f"grep -A 10 'teacher:' {config_file}",
            "Reading teacher configuration"
        )
        print("Teacher Config:")
        print(output)
    else:
        print(f"‚úó Config file not found: {config_file}")


def analyze_log(log_file):
    """ÂàÜÊûêËÆ≠ÁªÉÊó•Âøó"""
    print_section(f"6. Log Analysis: {log_file}")
    
    if not Path(log_file).exists():
        print(f"‚úó Log file not found: {log_file}")
        return
    
    print("‚úì Found log file")
    
    # Êü•ÊâæÂÖ≥ÈîÆÈîôËØØ
    print("\n[*] Searching for errors...")
    output = run_command(
        f"grep -i 'error\\|failed\\|timeout\\|exception' {log_file} | head -20",
        ""
    )
    if output.strip():
        print("Errors found:")
        print(output)
    else:
        print("No obvious errors found")
    
    # Êü•ÊâæÊùÉÈáçÂêåÊ≠•Êó•Âøó
    print("\n[*] Searching for weight sync logs...")
    output = run_command(
        f"grep 'weight sync\\|collective' {log_file} | tail -20",
        ""
    )
    if output.strip():
        print("Weight sync logs:")
        print(output)
    else:
        print("No weight sync logs found")
    
    # Êü•ÊâæÊúÄÂêéÁöÑÊó•Âøó
    print("\n[*] Last 30 lines of log:")
    output = run_command(f"tail -30 {log_file}", "")
    print(output)


def generate_recommendations():
    """ÁîüÊàêÂª∫ËÆÆ"""
    print_section("7. Recommendations")
    
    print("""
Â¶ÇÊûúÈÅáÂà∞Âç°Ê≠ªÈóÆÈ¢òÔºåËØ∑Êåâ‰ª•‰∏ãÊ≠•È™§ÊéíÊü•Ôºö

1. üîç Ê£ÄÊü•Êó•Âøó‰∏≠ÁöÑ "Rollout weight sync" ÈÉ®ÂàÜ
   - Â¶ÇÊûúÁúãÂà∞ "timeout" ‚Üí Â¢ûÂä† actor_rollout_ref.nccl_timeout
   - Â¶ÇÊûúÁúãÂà∞ "error" ‚Üí Ê£ÄÊü• GPU ÂíåÁΩëÁªúËøûÊé•

2. üíæ Ê£ÄÊü• GPU ÂÜÖÂ≠ò
   - ËøêË°å nvidia-smiÔºåÊü•ÁúãÊòØÂê¶Êúâ GPU Ë¢´Âç°‰ΩèÊàñÂÜÖÂ≠òÊ∫¢Âá∫
   - ËÄÉËôëÂáèÂ∞ë rollout.n_gpus_per_node

3. üîß Ë∞ÉÊï¥ÈÖçÁΩÆ
   - Â¢ûÂä† nccl_timeout: actor_rollout_ref.nccl_timeout=1200
   - ÂáèÂ∞ëÂπ∂Ë°å: rollout.n_gpus_per_node=2
   - ÂêØÁî®Ë∞ÉËØï: export NCCL_DEBUG=INFO

4. üìä ÂêØÁî®ÂÆåÊï¥ËØäÊñ≠
   export NCCL_DEBUG=TRACE
   export TORCH_DISTRIBUTED_DEBUG=INFO
   export VLLM_LOGGING_LEVEL=DEBUG

5. üöÄ ÈáçÊñ∞ËøêË°åËÆ≠ÁªÉ
   nohup python3 -m recipe.gkd.main_gkd \\
     --config-path=recipe/gkd/config \\
     --config-name=on_policy_distill_trainer \\
     actor_rollout_ref.model.path=/path/to/model \\
     ... \\
     actor_rollout_ref.nccl_timeout=600 \\
     > train.log 2>&1 &
""")


def main():
    """‰∏ªÂáΩÊï∞"""
    print("\n" + "="*60)
    print("  GKD Deadlock Diagnosis Tool")
    print("  " + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("="*60)
    
    # 1. GPU Ê£ÄÊü•
    check_nvidia_gpu()
    
    # 2. Ray ÈõÜÁæ§Ê£ÄÊü•
    check_ray_cluster()
    
    # 3. PyTorch ÂàÜÂ∏ÉÂºèÊ£ÄÊü•
    check_torch_distributed()
    
    # 4. VERL Ê®°ÂùóÊ£ÄÊü•
    check_verl_imports()
    
    # 5. ÈÖçÁΩÆÊñá‰ª∂Ê£ÄÊü•
    check_config()
    
    # 6. Êó•ÂøóÂàÜÊûê
    log_file = "train.log"
    if len(sys.argv) > 1:
        log_file = sys.argv[1]
    
    if Path(log_file).exists():
        analyze_log(log_file)
    else:
        print_section(f"6. Log Analysis")
        print(f"‚ÑπÔ∏è  Log file not found: {log_file}")
        print(f"   Usage: python3 diagnose.py <log_file>")
    
    # 7. Âª∫ËÆÆ
    generate_recommendations()
    
    print("\n" + "="*60)
    print("  Diagnosis Complete")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
